[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "evalx"
description = "Next-generation evaluation framework for LLM applications with research-grade validation and production-ready performance"
authors = [
    {name = "EvalX Team", email = "team@evalx.ai"}
]
readme = "README.md"
license = {text = "MIT"}
dynamic = ["version"]
requires-python = ">=3.9"
keywords = ["llm", "evaluation", "metrics", "ai", "nlp", "machine-learning"]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Libraries :: Python Modules",
]

dependencies = [
    # Core dependencies - pinned for stability
    "numpy>=1.21.0,<2.0.0",
    "pandas>=1.3.0,<3.0.0",
    "scipy>=1.7.0,<2.0.0",
    "scikit-learn>=1.0.0,<2.0.0",
    
    # NLP and ML - compatible versions
    "transformers>=4.20.0,<5.0.0",
    "sentence-transformers>=2.2.0,<3.0.0",
    "torch>=1.12.0,<3.0.0",
    "nltk>=3.7,<4.0.0",
    "spacy>=3.4.0,<4.0.0",
    
    # Evaluation metrics - stable versions
    "rouge-score>=0.1.0,<1.0.0",
    "bert-score>=0.3.0,<1.0.0",
    
    # LLM integrations - latest stable
    "openai>=1.0.0,<2.0.0",
    "anthropic>=0.3.0,<1.0.0",
    "langchain>=0.1.0,<1.0.0",
    "langchain-openai>=0.1.0,<1.0.0",
    "langchain-anthropic>=0.1.0,<1.0.0",
    "langsmith>=0.1.0,<1.0.0",
    
    # Async and performance - stable versions
    "aiohttp>=3.8.0,<4.0.0",
    "tenacity>=8.0.0,<9.0.0",
    
    # Statistics and analysis - pinned majors
    "statsmodels>=0.13.0,<1.0.0",
    "matplotlib>=3.5.0,<4.0.0",
    "seaborn>=0.11.0,<1.0.0",
    "plotly>=5.0.0,<6.0.0",
    
    # Utilities - stable versions
    "pydantic>=2.0.0,<3.0.0",
    "typer>=0.9.0,<1.0.0",
    "rich>=13.0.0,<14.0.0",
    "tqdm>=4.64.0,<5.0.0",
    "python-dotenv>=1.0.0,<2.0.0",
    "pyyaml>=6.0,<7.0",
    "jsonschema>=4.0.0,<5.0.0",
    
    # Multimodal capabilities - compatible versions
    "pillow>=9.0.0,<11.0.0",
    "opencv-python>=4.5.0,<5.0.0",
    "librosa>=0.9.0,<1.0.0",
    "datasets>=2.0.0,<3.0.0",
    "evaluate>=0.4.0,<1.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0,<8.0.0",
    "pytest-asyncio>=0.21.0,<1.0.0",
    "pytest-cov>=4.0.0,<5.0.0",
    "black>=22.0.0,<25.0.0",
    "isort>=5.10.0,<6.0.0",
    "flake8>=5.0.0,<8.0.0",
    "mypy>=1.0.0,<2.0.0",
    "pre-commit>=2.20.0,<4.0.0",
]

research = [
    "jupyter>=1.0.0,<2.0.0",
    "ipywidgets>=8.0.0,<9.0.0",
    "datasets>=2.0.0,<3.0.0",
    "wandb>=0.13.0,<1.0.0",
    "mlflow>=2.0.0,<3.0.0",
]

production = [
    "redis>=4.0.0,<6.0.0",
    "celery>=5.2.0,<6.0.0",
    "prometheus-client>=0.15.0,<1.0.0",
    "sentry-sdk>=1.0.0,<2.0.0",
]

all = [
    "evalx[dev,research,production]"
]

[project.urls]
Homepage = "https://github.com/evalx-ai/evalx"
Documentation = "https://evalx.readthedocs.io"
Repository = "https://github.com/evalx-ai/evalx"
Issues = "https://github.com/evalx-ai/evalx/issues"

[project.scripts]
evalx = "evalx.cli:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["evalx*"]

[tool.setuptools.dynamic]
version = {attr = "evalx.__version__"}

[tool.setuptools.package-data]
evalx = ["py.typed", "*.yaml", "*.json", "*.md"]

[tool.black]
line-length = 88
target-version = ['py39']
include = '\.pyi?$'

[tool.isort]
profile = "black"
multi_line_output = 3
line_length = 88

[tool.mypy]
python_version = "3.9"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--cov=evalx",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "llm: marks tests that require LLM API access",
]
